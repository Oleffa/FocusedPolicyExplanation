#!/usr/bin/env python

"""
This example demonstrates the basic use of the explanation generation framework.
The figures in the paper wer generated using this code.
"""


import numpy as np
# Import the Explanation Generation Framework and all helper functions
import sys
#sys.path.append('explanation_generator')
#from policy_explanation_generator import explanation_generator
#from generate_state_space import DimensionGenerator
from explanation_generator import Action, ExplanationGenerator, DimensionGenerator, Heatmap_2D
import matplotlib.pyplot as plt

plots_directory = "/home/oli/Workspace/FocusedPolicyExplanation/plots"
classifier_directory = "/home/oli/Workspace/FocusedPolicyExplanation/classifiers"

# Generate the dimensions
dg = DimensionGenerator(classifier_storage_path=classifier_directory)

dg.clear_classifiers() # Clear the classifier folder so that old dimension-
                        # classifiers are not used on accident
                        # (comment out if using custom pkl files)


# 1. Generate dimensions
# First one needs to generate the dimensions consisting of a set of descriptors a name
# and a natural language representation

# Dimension 1 
descriptor_names = ["low", "middle", "high"]
limits = [0,0.33,0.66,1]
dg.generate_dimension(name="Dimension 1", nlr="Dimension 1 was", descriptor_names=descriptor_names, max_range=1.0, min_range=0.0, save=True, limits=limits, duration=300, automated=True, normalized=True, savefig=False, plot=True)

# Dimension 2
descriptor_names = ["low", "high"]
limits = [0,0.6,1]
dg.generate_dimension(name="Dimension 2", nlr="Dimension 2 was", descriptor_names=descriptor_names, max_range=1.0, min_range=0.0, save=True, limits=limits, duration=300, automated=True, normalized=True, savefig=False, plot=True)

# 2. Generate the actions

a0 = Action("Action 1", 0)
a1 = Action("Action 2", 1)
a2 = Action("Action 3", 2)
actions = [a0,a1,a2]
# 2. Make the policy

# Define a simple policy at 

class Policy:
    def getAction(self, state):
        if state[0] < 0.5:
            if state[1] > 0.6:
                return 1
            else:
                return 0
        else:
            if state[1] > state[0]+0.1:
                return 1
            else:
                return 0
    def getPolicyY(self,x):
        if x < 0.5:
            return 0.6
        else:
            return x+0.1

#=========================================================================
# TODO make a plot for when which dimension is used for explaining.
# TODO make a list with all explanations SORTED by relevance or (others)
# TODO add colorbars to savefig and redo the plots (because the legends are still missing)
# TODO draw the bands in the plots
#=========================================================================
# Instantiate the explanation generator. Uses the pkl files generated by
# the dimension generator stored in the classifier directory
policy = Policy()
eg = ExplanationGenerator(actions, policy, classifier_directory)

# Test: Plot the state space, a test state, the policy and the descriptor boundaries
eg.plot2D([0.2, 0.8],0,1)

# Plot a 2d heatmap of explanation quality measurements, this generates the plots in the paper (Fig 2 and 3)
heatmap = Heatmap_2D(plots_directory)
heatmap.plot(5,5, 0, 1, eg, policy, actions, entropy_threshold=0.9, threshold=[0.6, 0.6, 0.6], interpolation=False, show_all_concepts=False, show_all_policies=False, savefig=False)
#heatmap.plot_single(5,5, 0, 1, eg, policy, actions, entropy_threshold=0.96, threshold=[0.6, 0.6, 0.6], interpolation=False, show_all_concepts=False, show_all_policies=False, savefig=False)

# Generate the natural language explanation with the quality measures
#print(eg.generate_natural_language_explanation(state[3], actions, threshold=[0.7, 0.6, 0.7], entropy_threshold=0.85,N=2,verbose=True))

